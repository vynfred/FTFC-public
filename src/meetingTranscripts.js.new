const functions = require('firebase-functions');
const admin = require('firebase-admin');
const { google } = require('googleapis');
const { Storage } = require('@google-cloud/storage');
const speech = require('@google-cloud/speech');
const fs = require('fs');
const os = require('os');
const path = require('path');

// Initialize Firebase Admin if not already initialized
if (!admin.apps.length) {
  admin.initializeApp();
}

const db = admin.firestore();

// Initialize Storage with service account credentials
const storage = new Storage({
  projectId: functions.config().google.project_id,
  credentials: {
    client_email: functions.config().google.client_email,
    private_key: functions.config().google.private_key.replace(/\\n/g, '\n')
  }
});

// Initialize Speech-to-Text client with service account credentials
const speechClient = new speech.SpeechClient({
  projectId: functions.config().google.project_id,
  credentials: {
    client_email: functions.config().google.client_email,
    private_key: functions.config().google.private_key.replace(/\\n/g, '\n')
  }
});

/**
 * Process a Google Meet recording and generate a transcript
 *
 * This function is triggered by a webhook from Google Meet when a recording is available
 */
exports.processMeetRecording = functions.https.onRequest(async (req, res) => {
  // Verify the request is from Google (in production, implement proper authentication)
  // For now, we'll use a simple API key check
  const apiKey = req.headers['x-api-key'];
  if (apiKey !== functions.config().google.webhook_key) {
    console.error('Unauthorized request');
    return res.status(401).send('Unauthorized');
  }

  try {
    const { meetingId, recordingId, entityType, entityId } = req.body;

    if (!meetingId || !recordingId || !entityType || !entityId) {
      console.error('Missing required parameters');
      return res.status(400).send('Missing required parameters');
    }

    // Log the request
    console.log(`Processing recording for meeting ${meetingId}, entity ${entityType}/${entityId}`);

    // Queue the processing job
    await db.collection('transcriptionJobs').add({
      meetingId,
      recordingId,
      entityType,
      entityId,
      status: 'queued',
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    // Return success immediately, processing will happen in the background
    return res.status(200).send('Processing queued');
  } catch (error) {
    console.error('Error processing webhook:', error);
    return res.status(500).send('Internal server error');
  }
});

/**
 * Background function to process transcription jobs
 *
 * This function runs on a schedule to process queued transcription jobs
 */
exports.processTranscriptionQueue = async (context) => {
  try {
    // Get queued jobs
    const jobsSnapshot = await db.collection('transcriptionJobs')
      .where('status', '==', 'queued')
      .orderBy('createdAt')
      .limit(5) // Process 5 jobs at a time
      .get();

    if (jobsSnapshot.empty) {
      console.log('No transcription jobs to process');
      return null;
    }

    // Process each job
    const jobs = [];
    jobsSnapshot.forEach(doc => {
      jobs.push({
        id: doc.id,
        ...doc.data()
      });
    });

    for (const job of jobs) {
      // Update job status to processing
      await db.collection('transcriptionJobs').doc(job.id).update({
        status: 'processing',
        processingStartedAt: admin.firestore.FieldValue.serverTimestamp()
      });

      try {
        // Get the meeting details
        const meetingDoc = await db.collection('meetings').doc(job.meetingId).get();
        if (!meetingDoc.exists) {
          throw new Error(`Meeting ${job.meetingId} not found`);
        }

        const meeting = meetingDoc.data();

        // Get the recording from Google Drive
        const recording = await downloadRecording(job.recordingId);

        // Generate transcript
        const transcript = await generateTranscript(recording);

        // Save transcript to Firestore
        const transcriptRef = await db.collection('transcripts').add({
          meetingId: job.meetingId,
          title: meeting.title || 'Meeting Transcript',
          date: meeting.date,
          participants: meeting.attendees || [],
          transcript: transcript.fullText,
          summary: await generateSummary(transcript.fullText),
          keyPoints: await extractKeyPoints(transcript.fullText),
          actionItems: await extractActionItems(transcript.fullText),
          entityType: job.entityType,
          entityId: job.entityId,
          createdAt: admin.firestore.FieldValue.serverTimestamp(),
          updatedAt: admin.firestore.FieldValue.serverTimestamp()
        });

        // Update the entity with reference to the transcript
        const entityRef = db.collection(`${job.entityType}s`).doc(job.entityId);
        const entityDoc = await entityRef.get();

        if (entityDoc.exists) {
          const entityData = entityDoc.data();
          const transcripts = entityData.transcripts || [];

          await entityRef.update({
            transcripts: [...transcripts, {
              id: transcriptRef.id,
              title: meeting.title || 'Meeting Transcript',
              date: meeting.date,
              meetingId: job.meetingId
            }],
            updatedAt: admin.firestore.FieldValue.serverTimestamp()
          });
        }

        // Create activity log entry
        await db.collection('activity').add({
          type: 'transcript',
          action: 'created',
          transcriptId: transcriptRef.id,
          entityType: job.entityType,
          entityId: job.entityId,
          title: meeting.title || 'Meeting Transcript',
          timestamp: admin.firestore.FieldValue.serverTimestamp(),
          description: `New meeting transcript added: ${meeting.title || 'Meeting Transcript'}`
        });

        // Update job status to completed
        await db.collection('transcriptionJobs').doc(job.id).update({
          status: 'completed',
          transcriptId: transcriptRef.id,
          completedAt: admin.firestore.FieldValue.serverTimestamp()
        });

        console.log(`Successfully processed transcript for meeting ${job.meetingId}`);
      } catch (error) {
        console.error(`Error processing job ${job.id}:`, error);

        // Update job status to failed
        await db.collection('transcriptionJobs').doc(job.id).update({
          status: 'failed',
          error: error.message,
          failedAt: admin.firestore.FieldValue.serverTimestamp()
        });
      }
    }

    return null;
  } catch (error) {
    console.error('Error processing transcription queue:', error);
    return null;
  }
};

/**
 * Download a recording from Google Drive
 * @param {string} recordingId - The ID of the recording in Google Drive
 * @returns {Promise<string>} - Path to the downloaded file
 */
async function downloadRecording(recordingId) {
  try {
    // Authenticate with Google Drive using service account
    const serviceAccount = {
      client_email: functions.config().google.client_email,
      private_key: functions.config().google.private_key.replace(/\\n/g, '\n')
    };

    const auth = new google.auth.JWT(
      serviceAccount.client_email,
      null,
      serviceAccount.private_key,
      ['https://www.googleapis.com/auth/drive.readonly']
    );

    const drive = google.drive({ version: 'v3', auth });

    // Get file metadata
    const fileMetadata = await drive.files.get({
      fileId: recordingId,
      fields: 'name,mimeType'
    });

    // Create temp file path
    const tempFilePath = path.join(os.tmpdir(), fileMetadata.data.name);

    // Download file
    const response = await drive.files.get({
      fileId: recordingId,
      alt: 'media'
    }, { responseType: 'stream' });

    // Save to temp file
    const dest = fs.createWriteStream(tempFilePath);

    return new Promise((resolve, reject) => {
      response.data
        .on('error', err => {
          reject(err);
        })
        .pipe(dest)
        .on('error', err => {
          reject(err);
        })
        .on('finish', () => {
          resolve(tempFilePath);
        });
    });
  } catch (error) {
    console.error('Error downloading recording:', error);
    throw error;
  }
}

/**
 * Generate a transcript from an audio file
 * @param {string} audioFilePath - Path to the audio file
 * @returns {Promise<Object>} - Transcript data
 */
async function generateTranscript(audioFilePath) {
  try {
    // Read the audio file
    const audioBytes = fs.readFileSync(audioFilePath).toString('base64');

    // Configure the request
    const audio = {
      content: audioBytes
    };
    const config = {
      encoding: 'MP3',
      sampleRateHertz: 16000,
      languageCode: 'en-US',
      enableAutomaticPunctuation: true,
      enableSpeakerDiarization: true,
      diarizationSpeakerCount: 2, // Adjust based on expected number of speakers
      model: 'video'
    };

    // Detect speech
    const [response] = await speechClient.recognize({
      audio,
      config
    });

    // Process the response
    const transcription = response.results
      .map(result => result.alternatives[0].transcript)
      .join('\n');

    // Clean up temp file
    fs.unlinkSync(audioFilePath);

    return {
      fullText: transcription,
      segments: response.results.map(result => ({
        text: result.alternatives[0].transcript,
        confidence: result.alternatives[0].confidence
      }))
    };
  } catch (error) {
    console.error('Error generating transcript:', error);
    throw error;
  }
}

/**
 * Generate a summary of the transcript using AI
 * @param {string} transcript - The full transcript text
 * @returns {Promise<string>} - Summary of the transcript
 */
async function generateSummary(transcript) {
  // In a real implementation, you would use an AI service like OpenAI or Google's Vertex AI
  // For now, we'll return a placeholder
  return "This is an automatically generated summary of the meeting. In a real implementation, this would be generated using AI based on the transcript content.";
}

/**
 * Extract key points from the transcript using AI
 * @param {string} transcript - The full transcript text
 * @returns {Promise<Array<string>>} - List of key points
 */
async function extractKeyPoints(transcript) {
  // In a real implementation, you would use an AI service
  // For now, we'll return placeholders
  return [
    "Key point 1 would be extracted from the transcript",
    "Key point 2 would be extracted from the transcript",
    "Key point 3 would be extracted from the transcript"
  ];
}

/**
 * Extract action items from the transcript using AI
 * @param {string} transcript - The full transcript text
 * @returns {Promise<Array<Object>>} - List of action items
 */
async function extractActionItems(transcript) {
  // In a real implementation, you would use an AI service
  // For now, we'll return placeholders
  return [
    {
      description: "Action item 1 would be extracted from the transcript",
      assignee: null,
      dueDate: null
    },
    {
      description: "Action item 2 would be extracted from the transcript",
      assignee: null,
      dueDate: null
    }
  ];
}
